{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 11 Challenge\n",
    "## Deliverable 1: Scrape Titles and Preview Text from Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Splinter and BeautifulSoup\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading: 100%|████████████████████████████████████████████████████████| 6.79M/6.79M [00:00<00:00, 21.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Visit the Website\n",
    "\n",
    "1. Use automated browsing to visit the [Mars NASA news site](https://redplanetscience.com). Inspect the page to identify which elements to scrape.\n",
    "\n",
    "      > **Hint** To identify which elements to scrape, you might want to inspect the page by using Chrome DevTools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the Mars NASA news site: https://redplanetscience.com\n",
    "url = 'https://redplanetscience.com/'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Scrape the Website\n",
    "\n",
    "Create a Beautiful Soup object and use it to extract text elements from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Beautiful Soup object\n",
    "html = browser.html\n",
    "news_soup = soup(html, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3581695408.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\rache\\AppData\\Local\\Temp\\ipykernel_9876\\3581695408.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    text_elements = news_soup.find_all('div', _class__='list_text'))\u001b[0m\n\u001b[1;37m                                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Extract all of the text elements - this was done incorrectly before\n",
    "text_elements = news_soup.find_all('div', _class__='list_text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the dictionaries\n",
    "results_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Store the Results\n",
    "\n",
    "Extract the titles and preview text of the news articles that you scraped. Store the scraping results in Python data structures as follows:\n",
    "\n",
    "* Store each title-and-preview pair in a Python dictionary. And, give each dictionary two keys: `title` and `preview`. An example is the following:\n",
    "\n",
    "  ```python\n",
    "  {'title': \"Mars Rover Begins Mission!\", \n",
    "        'preview': \"NASA's Mars Rover begins a multiyear mission to collect data about the little-explored planet.\"}\n",
    "  ```\n",
    "\n",
    "* Store all the dictionaries in a Python list.\n",
    "\n",
    "* Print the list in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pane in text_elements:\n",
    "    titles = pane.find('div', class_=\"content_title\").text\n",
    "    previews = pane.find('div', class_=\"article_teaser_body\").text\n",
    "    elements_dict = {}\n",
    "    elements_dict['title'] = titles\n",
    "    elements_dict['preview'] = previews\n",
    "    results_list.append(elements_dict)\n",
    "    \n",
    "\n",
    "\n",
    "# for div in divs:\n",
    "#     titles = div.find_all('div', class_=\"content_title\").text\n",
    "#     previews = div.find_all('div', class_=\"article_teaser_body\").text\n",
    "\n",
    "#     elements_dict = {\n",
    "#         \"title\": titles,\n",
    "#         \"preview\": previews\n",
    "#     }\n",
    "#     results_list.append(elements_dict)\n",
    "\n",
    "\n",
    "#     previews = html_soup.find_all('div', class_='article_teaser_body')\n",
    "#     for title in titles:\n",
    "#     titles_text = title.text\n",
    "#     for preview in previews:\n",
    "#         previews_text = preview.text\n",
    "        \n",
    "#         elements_dict = {\n",
    "#         \"title\": titles_text,\n",
    "#         \"preview\": previews_text\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print list to confirm results\n",
    "results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Step 4: Export the Data\n",
    "\n",
    "Optionally, store the scraped data in a file or database (to ease sharing the data with others). To do so, export the scraped data to either a JSON file or a MongoDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to JSON\n",
    "import json\n",
    "with open('results', 'w') as fout:\n",
    "    json.dump(results_list, fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1566d428ca80b67ab900cf9ddc77ca2502afa0b1a0e82ceae6ec270c0cac857f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
